{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle \n",
    "from natsort import natsorted\n",
    "\n",
    "def load_data_as_dict(directory_path):\n",
    "    \"\"\"\n",
    "    Return pickle file in directory_path as a list. \n",
    "    Also returns a list of all the unique id's in the dataset. \n",
    "    Why do we need unique id? \n",
    "        Lets say a task is randomly mapped 100 times. \n",
    "        All the 100 data (dict in pickle file) will have the same id. \n",
    "        So it is easier to retrieve all the mapping for a single task\n",
    "    \"\"\"\n",
    "    entries = os.listdir(directory_path)\n",
    "    files = natsorted([entry for entry in entries if os.path.isfile(os.path.join(directory_path, entry))])\n",
    "\n",
    "    list_of_dicts = []\n",
    "    list_of_uuids = []\n",
    "\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data_dict = pickle.load(file)\n",
    "            list_of_dicts.append(data_dict)\n",
    "\n",
    "            uuid = data_dict['task_dag'].id\n",
    "\n",
    "            if uuid not in list_of_uuids:\n",
    "                list_of_uuids.append(uuid)\n",
    "\n",
    "    return list_of_dicts, list_of_uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, _ = load_data_as_dict('data/task_from_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delay': int}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "dataset[0]['task_dag'].graph\n",
    "def get_node_attribute_types(G):\n",
    "    attribute_types = {}\n",
    "\n",
    "    for node, attributes in G.nodes(data=True):\n",
    "        for attr_name, attr_value in attributes.items():\n",
    "            attribute_types[attr_name] = type(attr_value)\n",
    "\n",
    "    return attribute_types\n",
    "\n",
    "get_node_attribute_types(dataset[0]['task_dag'].graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    81200.000000\n",
       "mean      2040.257993\n",
       "std        354.290683\n",
       "min       1027.000000\n",
       "25%       1738.000000\n",
       "50%       2033.000000\n",
       "75%       2322.000000\n",
       "max       3672.000000\n",
       "Name: network_processing_time, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(dataset)\n",
    "df['network_processing_time'] = pd.to_numeric(df['network_processing_time'])\n",
    "df['network_processing_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data/task_from_graph_tensor' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch\n",
    "\n",
    "def convert_graph_to_tensor(graph, latency):\n",
    "    # graph_tensor = from_networkx(graph, group_node_attrs=['weight', 'type'])\n",
    "    graph_tensor = from_networkx(graph)\n",
    "    graph_tensor.y = torch.tensor([latency])\n",
    "    # graph_tensor.x[:, 0] = graph_tensor.x[:, 0] / 100\n",
    "    # del graph_tensor.pos\n",
    "    return graph_tensor\n",
    "\n",
    "def convert_edge_index(edge_index, num_of_tasks):\n",
    "    converted_edge_index = []\n",
    "    node_mapping = {'Start': 0, 'Exit': num_of_tasks}\n",
    "\n",
    "    for src, dest in edge_index:\n",
    "        if src == 'Start':\n",
    "            src = node_mapping[src]\n",
    "\n",
    "        if dest == 'Exit':\n",
    "            dest = node_mapping[dest]\n",
    "\n",
    "        converted_edge_index.append((int(src), int(dest)))\n",
    "\n",
    "    return converted_edge_index\n",
    "\n",
    "directory_path = 'data/task_from_graph_tensor'\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory '{directory_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' already exists.\")\n",
    "\n",
    "for idx, data in enumerate(dataset):\n",
    "    \n",
    "    task_dag = data['task_dag']\n",
    "    task_processing_time = float(data['network_processing_time'])\n",
    "    target_value = torch.tensor([task_processing_time]).float()\n",
    "\n",
    "    task_graph = task_dag.graph\n",
    "    edge_index = list(task_graph.edges)\n",
    "    \n",
    "    total_tasks = len(task_graph.nodes)\n",
    "    last_task = len(task_graph.nodes) - 1\n",
    "    \n",
    "    converted_edge_index = convert_edge_index(edge_index, last_task)\n",
    "    converted_edge_index_torch = torch.tensor(converted_edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    dummy_input = torch.ones(total_tasks).view(-1,1)\n",
    "    data = Data(x=dummy_input,edge_index=converted_edge_index_torch, y=target_value)\n",
    "\n",
    "    # print(data.x.shape)\n",
    "    # break\n",
    "    torch.save(data, f'{directory_path}/graph_{idx}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
